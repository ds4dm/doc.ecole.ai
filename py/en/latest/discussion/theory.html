<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Ecole Theoretical Model &mdash; Ecole 0.8.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Contribution Guidelines" href="../contributing.html" />
    <link rel="prev" title="Seeding" href="seeding.html" />
<script>
	window.goatcounter = { path: function(p) { return location.host + p } }
</script>
<script data-goatcounter="https://ecoleai.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/ecole-logo-bare.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.8
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../using-environments.html">Using Environments</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How to</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../howto/observation-functions.html">Use Observation Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../howto/reward-functions.html">Use Reward Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../howto/create-functions.html">Create New Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../howto/create-environments.html">Create New Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../howto/instances.html">Generate Problem Instances</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Practical Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/ds4dm/ecole/tree/master/examples/configuring-bandits/example.ipynb">Configuring the Solver with Bandits</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/ds4dm/ecole/tree/master/examples/branching-imitation/example.ipynb">Branching with Imitation Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/environments.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/observations.html">Observations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/rewards.html">Rewards</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/information.html">Informations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/scip-interface.html">SCIP Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/instances.html">Instance Generators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/utilities.html">Utilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Discussion</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gym-differences.html">Differences with OpenAI Gym</a></li>
<li class="toctree-l1"><a class="reference internal" href="seeding.html">Seeding</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Ecole Theoretical Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#markov-decision-process">Markov Decision Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-mdp-control-problem">The MDP Control Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Partially-Observable Markov Decision Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-po-mdp-control-problem">The PO-MDP Control Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ecole-as-po-mdp-elements">Ecole as PO-MDP Elements</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Zone</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developers/example-observation.html">Example: How to Contribute an Observation Function</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Ecole</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Ecole Theoretical Model</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ds4dm/ecole/blob/master/docs/discussion/theory.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ecole-theoretical-model">
<span id="theory"></span><h1>Ecole Theoretical Model<a class="headerlink" href="#ecole-theoretical-model" title="Permalink to this headline"></a></h1>
<p>The Ecole elements directly correspond to the different elements of
an episodic <a class="reference external" href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">partially-observable Markov decision process</a>
(PO-MDP).</p>
<section id="markov-decision-process">
<h2>Markov Decision Process<a class="headerlink" href="#markov-decision-process" title="Permalink to this headline"></a></h2>
<p>Consider a regular Markov decision process
<span class="math notranslate nohighlight">\((\mathcal{S}, \mathcal{A}, p_\textit{init}, p_\textit{trans}, R)\)</span>,
whose components are</p>
<ul class="simple">
<li><p>a state space <span class="math notranslate nohighlight">\(\mathcal{S}\)</span></p></li>
<li><p>an action space <span class="math notranslate nohighlight">\(\mathcal{A}\)</span></p></li>
<li><p>an initial state distribution <span class="math notranslate nohighlight">\(p_\textit{init}: \mathcal{S} \to \mathbb{R}_{\geq 0}\)</span></p></li>
<li><p>a state transition distribution
<span class="math notranslate nohighlight">\(p_\textit{trans}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \mathbb{R}_{\geq 0}\)</span></p></li>
<li><p>a reward function <span class="math notranslate nohighlight">\(R: \mathcal{S} \to \mathbb{R}\)</span>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Having deterministic rewards <span class="math notranslate nohighlight">\(r_t = R(s_t)\)</span> is an arbitrary choice
here, in order to best fit the Ecole library. It is not restrictive though,
as any MDP with stochastic rewards
<span class="math notranslate nohighlight">\(r_t \sim p_\textit{reward}(r_t|s_{t-1},a_{t-1},s_{t})\)</span>
can be converted into an equivalent MDP with deterministic ones,
by considering the reward as part of the state.</p>
</div>
<p>Together with an action policy</p>
<div class="math notranslate nohighlight">
\[\pi: \mathcal{A} \times \mathcal{S} \to \mathbb{R}_{\geq 0}\]</div>
<p>such that <span class="math notranslate nohighlight">\(a_t \sim \pi(a_t|s_t)\)</span>, an MDP can be unrolled to produce
state-action trajectories</p>
<div class="math notranslate nohighlight">
\[\tau=(s_0,a_0,s_1,\dots)\]</div>
<p>that obey the following joint distribution</p>
<div class="math notranslate nohighlight">
\[\tau \sim \underbrace{p_\textit{init}(s_0)}_{\text{initial state}}
\prod_{t=0}^\infty \underbrace{\pi(a_t | s_t)}_{\text{next action}}
\underbrace{p_\textit{trans}(s_{t+1} | a_t, s_t)}_{\text{next state}}
\text{.}\]</div>
<section id="the-mdp-control-problem">
<h3>The MDP Control Problem<a class="headerlink" href="#the-mdp-control-problem" title="Permalink to this headline"></a></h3>
<p>We define the MDP control problem as that of finding a policy
<span class="math notranslate nohighlight">\(\pi^\star\)</span> which is optimal with respect to the expected total
reward,</p>
<div class="math notranslate nohighlight" id="equation-mdp-control">
<span class="eqno">(1)<a class="headerlink" href="#equation-mdp-control" title="Permalink to this equation"></a></span>\[\pi^\star = \underset{\pi}{\operatorname{arg\,max}}
\lim_{T \to \infty} \mathbb{E}_\tau\left[\sum_{t=0}^{T} r_t\right]
\text{,}\]</div>
<p>where <span class="math notranslate nohighlight">\(r_t := R(s_t)\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the general case this quantity may not be bounded, for example for MDPs
corresponding to <em>continuing</em> tasks where episode length may be infinite.
In Ecole, we guarantee that all environments correspond to <em>episodic</em>
tasks, that is, each episode is guaranteed to end in a terminal state.
This can be modeled by introducing a null state <span class="math notranslate nohighlight">\(s_\textit{null}\)</span>,
such that</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s_\textit{null}\)</span> is absorbing, i.e., <span class="math notranslate nohighlight">\(p_\textit{trans}(s_{t+1}|a_t,s_t=s_\textit{null}) := \delta_{s_\textit{null}}(s_{t+1})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(s_\textit{null}\)</span> yields no reward, i.e., <span class="math notranslate nohighlight">\(R(s_\textit{null}) := 0\)</span></p></li>
<li><p>a state <span class="math notranslate nohighlight">\(s\)</span> is terminal <span class="math notranslate nohighlight">\(\iff\)</span> it transitions
into the null state with probability one, i.e., <span class="math notranslate nohighlight">\(p_\textit{trans}(s_{t+1}|a_t,s_t=s) := \delta_{s_\textit{null}}(s_{t+1})\)</span></p></li>
</ul>
<p>As such, all actions and states encountered after a terminal state
can be safely ignored in the MDP control problem.</p>
</div>
</section>
</section>
<section id="id1">
<h2>Partially-Observable Markov Decision Process<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>In the PO-MDP setting, complete information about the current MDP state
is not necessarily available to the decision-maker. Instead,
at each step only a partial observation <span class="math notranslate nohighlight">\(o \in \Omega\)</span>
is made available, which can be seen as the result of applying an observation
function <span class="math notranslate nohighlight">\(O: \mathcal{S} \to \Omega\)</span> to the current state. As such, a
PO-MDP consists of a tuple
<span class="math notranslate nohighlight">\((\mathcal{S}, \mathcal{A}, p_\textit{init}, p_\textit{trans}, R, O)\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Similarly to having deterministic rewards, having deterministic
observations is an arbitrary choice here, but is not restrictive.</p>
</div>
<p>As a result, PO-MDP trajectories take the form</p>
<div class="math notranslate nohighlight">
\[\tau=(o_0,r_0,a_0,o_1\dots)
\text{,}\]</div>
<p>where <span class="math notranslate nohighlight">\(o_t:= O(s_t)\)</span> and <span class="math notranslate nohighlight">\(r_t:=R(s_t)\)</span> are respectively the
observation and the reward collected at time step <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>Let us now introduce a convenience variable
<span class="math notranslate nohighlight">\(h_t:=(o_0,r_0,a_0,\dots,o_t,r_t)\in\mathcal{H}\)</span> that represents the
PO-MDP history at time step <span class="math notranslate nohighlight">\(t\)</span>. Due to the non-Markovian nature of
the trajectories, that is,</p>
<div class="math notranslate nohighlight">
\[o_{t+1},r_{t+1} \mathop{\rlap{\perp}\mkern2mu{\not\perp}} h_{t-1} \mid o_t,r_t,a_t
\text{,}\]</div>
<p>the decision-maker must take into account the whole history of observations,
rewards and actions in order to decide on an optimal action at current time
step <span class="math notranslate nohighlight">\(t\)</span>. PO-MDP policies then take the form</p>
<div class="math notranslate nohighlight">
\[\pi:\mathcal{A} \times \mathcal{H} \to \mathbb{R}_{\geq 0}\]</div>
<p>such that <span class="math notranslate nohighlight">\(a_t \sim \pi(a_t|h_t)\)</span>.</p>
<section id="the-po-mdp-control-problem">
<h3>The PO-MDP Control Problem<a class="headerlink" href="#the-po-mdp-control-problem" title="Permalink to this headline"></a></h3>
<p>The PO-MDP control problem can then be written identically to the MDP one,</p>
<div class="math notranslate nohighlight" id="equation-pomdp-control">
<span class="eqno">(2)<a class="headerlink" href="#equation-pomdp-control" title="Permalink to this equation"></a></span>\[\pi^\star = \underset{\pi}{\operatorname{arg\,max}} \lim_{T \to \infty}
\mathbb{E}_\tau\left[\sum_{t=0}^{T} r_t\right]
\text{.}\]</div>
</section>
</section>
<section id="ecole-as-po-mdp-elements">
<h2>Ecole as PO-MDP Elements<a class="headerlink" href="#ecole-as-po-mdp-elements" title="Permalink to this headline"></a></h2>
<p>The following Ecole elements directly translate into PO-MDP elements from
the aforementioned formulation:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../reference/rewards.html#ecole.typing.RewardFunction" title="ecole.typing.RewardFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">RewardFunction</span></code></a> &lt;=&gt; <span class="math notranslate nohighlight">\(R\)</span></p></li>
<li><p><a class="reference internal" href="../reference/observations.html#ecole.typing.ObservationFunction" title="ecole.typing.ObservationFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">ObservationFunction</span></code></a> &lt;=&gt; <span class="math notranslate nohighlight">\(O\)</span></p></li>
<li><p><a class="reference internal" href="../reference/environments.html#ecole.typing.Dynamics.reset_dynamics" title="ecole.typing.Dynamics.reset_dynamics"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reset_dynamics()</span></code></a> &lt;=&gt;
<span class="math notranslate nohighlight">\(p_\textit{init}(s_0)\)</span></p></li>
<li><p><a class="reference internal" href="../reference/environments.html#ecole.typing.Dynamics.step_dynamics" title="ecole.typing.Dynamics.step_dynamics"><code class="xref py py-meth docutils literal notranslate"><span class="pre">step_dynamics()</span></code></a> &lt;=&gt;
<span class="math notranslate nohighlight">\(p_\textit{trans}(s_{t+1}|s_t,a_t)\)</span></p></li>
</ul>
<p>The state space <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> can be considered to be the whole computer
memory occupied by the environment, which includes the state of the underlying
SCIP solver instance. The action space <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is specific to each
environment.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In practice, both <a class="reference internal" href="../reference/rewards.html#ecole.typing.RewardFunction" title="ecole.typing.RewardFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">RewardFunction</span></code></a> and
<a class="reference internal" href="../reference/observations.html#ecole.typing.ObservationFunction" title="ecole.typing.ObservationFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">ObservationFunction</span></code></a> are implemented as stateful
classes, and therefore should be considered part of the MDP state
<span class="math notranslate nohighlight">\(s\)</span>. This <em>extended</em> state is not meant to take part in the MDP
dynamics per se, but nonetheless has to be considered as the actual
PO-MDP state, in order to allow for a strict interpretation of Ecole
environments as PO-MDPs.</p>
</div>
<p>The <a class="reference internal" href="../reference/environments.html#ecole.environment.Environment" title="ecole.environment.Environment"><code class="xref py py-class docutils literal notranslate"><span class="pre">Environment</span></code></a> class wraps all of
those components together to form the actual PO-MDP. Its API can be
interpreted as follows:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../reference/environments.html#ecole.environment.Environment.reset" title="ecole.environment.Environment.reset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reset()</span></code></a> &lt;=&gt;
<span class="math notranslate nohighlight">\(s_0 \sim p_\textit{init}(s_0), r_0=R(s_0), o_0=O(s_0)\)</span></p></li>
<li><p><a class="reference internal" href="../reference/environments.html#ecole.environment.Environment.step" title="ecole.environment.Environment.step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">step()</span></code></a> &lt;=&gt;
<span class="math notranslate nohighlight">\(s_{t+1} \sim p_\textit{trans}(s_{t+1}|a_t,s_t), r_t=R(s_t), o_t=O(s_t)\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">done</span> <span class="pre">==</span> <span class="pre">True</span></code> &lt;=&gt; the current state <span class="math notranslate nohighlight">\(s_{t}\)</span> is terminal. As such,
the episode ends now.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In Ecole we allow environments to optionally specify a set of valid
actions at each time step <span class="math notranslate nohighlight">\(t\)</span>. To this end, both the
<a class="reference internal" href="../reference/environments.html#ecole.environment.Environment.reset" title="ecole.environment.Environment.reset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reset()</span></code></a> and
<a class="reference internal" href="../reference/environments.html#ecole.environment.Environment.step" title="ecole.environment.Environment.step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">step()</span></code></a> methods return
the valid <code class="docutils literal notranslate"><span class="pre">action_set</span></code> for the next transition, in addition to the
current observation and reward. This action set is optional, and
environments in which the action set is implicit may simply return
<code class="docutils literal notranslate"><span class="pre">action_set</span> <span class="pre">==</span> <span class="pre">None</span></code>.</p>
</div>
<p>Implementation of both the PO-MDP policy <span class="math notranslate nohighlight">\(\pi(a_t|h_t)\)</span> and a method
to solve the resulting control problem <a class="reference internal" href="#equation-pomdp-control">(2)</a> is left to the
user.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As can be seen from <a class="reference internal" href="#equation-mdp-control">(1)</a> and <a class="reference internal" href="#equation-pomdp-control">(2)</a>, the initial
reward <span class="math notranslate nohighlight">\(r_0\)</span> returned by
<a class="reference internal" href="../reference/environments.html#ecole.environment.Environment.reset" title="ecole.environment.Environment.reset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reset()</span></code></a>
does not affect the control problem. In Ecole we
nevertheless chose to preserve this initial reward, in order to obtain
meaningful cumulated episode rewards, such as the total running time
(which must include the time spend in
<a class="reference internal" href="../reference/environments.html#ecole.environment.Environment.reset" title="ecole.environment.Environment.reset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reset()</span></code></a>), or the total
number of branch-and-bound nodes in a
<a class="reference internal" href="../reference/environments.html#ecole.environment.Branching" title="ecole.environment.Branching"><code class="xref py py-class docutils literal notranslate"><span class="pre">Branching</span></code></a> environment (which must include
the root node).</p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="seeding.html" class="btn btn-neutral float-left" title="Seeding" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../contributing.html" class="btn btn-neutral float-right" title="Contribution Guidelines" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Antoine Prouvost, Maxime Gasse, Didier Chételat, Justin Dumouchelle.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>